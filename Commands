# 1. Navigate to workspace and create virtual environment
cd /workspace
python3 -m venv venv
source venv/bin/activate

# 2. Clone your repository
git clone https://github.com/devasphn/DiaChat/
cd DiaChat

# 3. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 4. Start Ollama in background and pull model
ollama serve &
sleep 10  # Wait for service to start
ollama pull llama3.2

# 5. Update system packages
apt update
apt install -y git build-essential python3 python3-pip python3-venv ffmpeg libportaudio2 libsndfile1 wget

# 6. Install PyTorch with CUDA support first (critical for A40 GPU)
pip install torch==2.1.0 torchaudio==0.16.0 --index-url https://download.pytorch.org/whl/cu121

# 7. Install Dia model directly from GitHub
pip install git+https://github.com/nari-labs/dia.git

# 8. Install remaining requirements
pip install -r requirements.txt

# 9. Login to Hugging Face (required for Dia model access)
huggingface-cli login
# Enter your HF token when prompted

# 10. Create models directory and set environment variables
mkdir -p /workspace/models
export MODELS_DIR=/workspace/models
export HF_HUB_CACHE=/workspace/models
export OLLAMA_HOST=http://localhost:11434
export OLLAMA_MODEL=llama3.2:latest

# 11. Download Whisper model manually
wget -O /workspace/models/ggml-base.en.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin

# 12. Test GPU availability
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"

# 13. Run the application
python main.py
